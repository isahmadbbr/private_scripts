{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ICE-Nigeria-txt-and-xml-files_version_Nov_03_2015/ice-nig/txt - without speaker tags/written/novels/nov_01.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    text = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'ICE-Nigeria-txt-and-xml-files_version_Nov_03_2015/ice-nig/txt - without speaker tags/written/novels/nov_01.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "import pandas as pd\n",
    "# Split the text into rows\n",
    "rows = text.split('\\n')\n",
    "\n",
    "# Split each row into columns\n",
    "data = [row.split('\\t') for row in rows]\n",
    "\n",
    "# Convert the data to a pandas dataframe\n",
    "df = pd.DataFrame(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'unitary/toxic-bert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify a text\n",
    "def classify_toxicity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    is_toxic = probabilities[0][1] > 0.5\n",
    "    return is_toxic\n",
    "\n",
    "# Test the function\n",
    "print(classify_toxicity(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'deepcut.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    transcript = file.read()\n",
    "\n",
    "import re\n",
    "\n",
    "# Your transcript text\n",
    " \n",
    "\n",
    "\n",
    "# Define a regular expression pattern for timestamps with hours, minutes, and seconds\n",
    "timestamp_pattern = re.compile(r'\\d+:\\d+(:\\d+)?')\n",
    "\n",
    "# Remove timestamps from the text\n",
    "cleaned_text = re.sub(timestamp_pattern, '', transcript)\n",
    "\n",
    "# Remove empty lines\n",
    "cleaned_text = '\\n'.join(line for line in cleaned_text.splitlines() if line.strip())\n",
    "\n",
    "# Save the cleaned text to a file\n",
    "output_path = 'cleaned_transcript.txt'\n",
    "with open(output_path, 'w') as output_file:\n",
    "    output_file.write(cleaned_text)\n",
    "\n",
    "# Print the cleaned text\n",
    "#print(cleaned_text)\n",
    "print(f\"\\nCleaned text saved to '{output_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall Toxicity Percentage: 2.900961439768036\n"
     ]
    }
   ],
   "source": [
    "path = 'cleaned_transcript.txt'\n",
    "\n",
    "with open(path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Split the text into rows\n",
    "rows = text.split('\\n')\n",
    "\n",
    "# Split each row into columns\n",
    "data = [row.split('\\t') for row in rows]\n",
    "\n",
    "# Convert the data to a pandas dataframe\n",
    "df = pd.DataFrame(data, columns=[\"Text\"])\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'unitary/toxic-bert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify a text\n",
    "def classify_toxicity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    toxicity_percentage = probabilities[0][1].item() * 100\n",
    "    return toxicity_percentage\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "df['ToxicityPercentage'] = df['Text'].apply(classify_toxicity)\n",
    "\n",
    "# Calculate the overall percentage of toxicity for the entire document\n",
    "overall_toxicity_percentage = df['ToxicityPercentage'].mean()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Overall Toxicity Percentage: {overall_toxicity_percentage}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This code creates a DataFrame with columns for \"Text\" and \"Toxicity\" and saves it to a CSV file named 'toxicity_results.csv'. Adjust the output_csv_path variable to specify a different file path if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                  Text  Toxicity\n",
      "0                                            thank you  8.900988\n",
      "1                                            hey there  7.190707\n",
      "2                          love is a very strong thing  7.690600\n",
      "3                                        my sweetheart  1.698025\n",
      "4                               did I love you so much  5.644903\n",
      "..                                                 ...       ...\n",
      "667  until you get home don't start on  the roads t...  0.314047\n",
      "668                         oh let them be let them be  0.342521\n",
      "669                             to be together forever  4.373112\n",
      "670                             to build this would be  7.262250\n",
      "671                                             Shorts  0.385679\n",
      "\n",
      "[672 rows x 2 columns]\n",
      "\n",
      "Toxicity results saved to 'toxicity_results.csv'.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Read the cleaned transcript from file\n",
    "path = 'cleaned_transcript.txt'\n",
    "with open(path, 'r') as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Remove timestamps and empty lines\n",
    "timestamp_pattern = re.compile(r'\\d+:\\d+(:\\d+)?')\n",
    "cleaned_text = re.sub(timestamp_pattern, '', text)\n",
    "cleaned_text = '\\n'.join(line for line in cleaned_text.splitlines() if line.strip())\n",
    "\n",
    "# Split the text into rows\n",
    "rows = cleaned_text.split('\\n')\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'unitary/toxic-bert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify toxicity\n",
    "def classify_toxicity(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt')\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    toxicity_percentage = probabilities[0][1].item() * 100\n",
    "    return toxicity_percentage\n",
    "\n",
    "# Create a list to store results\n",
    "toxicity_data = []\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "for row in rows:\n",
    "    toxicity_percentage = classify_toxicity(row)\n",
    "    toxicity_data.append({\"Text\": row, \"Toxicity\": toxicity_percentage})\n",
    "\n",
    "# Create a DataFrame from the results\n",
    "df_toxicity = pd.DataFrame(toxicity_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "output_csv_path = 'toxicity_results.csv'\n",
    "df_toxicity.to_csv(output_csv_path, index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(df_toxicity)\n",
    "print(f\"\\nToxicity results saved to '{output_csv_path}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Toxicity Probability: 0.527604877948761\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torch.nn.functional import softmax\n",
    "\n",
    "def detect_toxicity(text, model, tokenizer):\n",
    "    # Tokenize input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "    # Forward pass, get logits\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "\n",
    "    # Apply softmax to get probabilities\n",
    "    probabilities = softmax(logits, dim=1)\n",
    "\n",
    "    # Get the probability of the toxic class\n",
    "    toxic_probability = probabilities[0, 1].item()\n",
    "\n",
    "    return toxic_probability\n",
    "\n",
    "# Load pretrained DistilBERT model and tokenizer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Example text\n",
    "text = \"he is a asshole\"\n",
    "\n",
    "# Detect toxicity\n",
    "toxicity_probability = detect_toxicity(text, model, tokenizer)\n",
    "\n",
    "# Print result\n",
    "print(f\"Toxicity Probability: {toxicity_probability}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0001, 0.8564]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "input_texts = [\"She is lovely\",\"he is a douchebag\"]\n",
    "results = toxicity.compute(predictions=input_texts)\n",
    "print([round(s, 4) for s in results[\"toxicity\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Text  Toxicity\n",
      "0  A short story by Muhd Ibrahim Muhsin\\nGwaggo c...    0.0709\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "\n",
    "# Specify the path to your text file\n",
    "file_path = 'ICE-Nigeria-txt-and-xml-files_version_Nov_03_2015/ice-nig/txt - without speaker tags/written/novels/nov_01.txt'\n",
    "\n",
    "\n",
    "# Read the text from the file into a DataFrame\n",
    "df = pd.DataFrame({'Text': [read_text_from_file(file_path)]})\n",
    "\n",
    "# Compute toxicity for each text in the DataFrame\n",
    "results = toxicity.compute(predictions=df['Text'].tolist())\n",
    "df['Toxicity'] = [round(score, 4) for score in results[\"toxicity\"]]\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Folder containing the text files\n",
    "folder_path = 'ICE-Nigeria-txt-and-xml-files_version_Nov_03_2015/ice-nig/txt - without speaker tags/written/novels/'\n",
    "\n",
    "# Load pre-trained model and tokenizer\n",
    "model_name = 'unitary/toxic-bert'\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Define a function to classify toxicity\n",
    "def classify_toxicity(text):\n",
    "    try:\n",
    "        inputs = tokenizer(text, return_tensors='pt')\n",
    "        outputs = model(**inputs)\n",
    "        probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "        toxicity_percentage = probabilities[0][1].item() * 100\n",
    "        return toxicity_percentage\n",
    "    except Exception as e:\n",
    "        # Handle errors during classification\n",
    "        print(f\"Error classifying text: {e}\")\n",
    "        return None\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['File', 'Text', 'ToxicityPercentage'])\n",
    "\n",
    "# Iterate through each text file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the content of the file\n",
    "        with open(file_path, 'r') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Classify toxicity and store results in DataFrame\n",
    "        toxicity_percentage = classify_toxicity(text)\n",
    "        results_df = results_df.append({'File': filename, 'Text': text, 'ToxicityPercentage': toxicity_percentage}, ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('toxicity_results.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n",
      "/var/folders/s1/rqvf1f_d5zb106l4ym22x_6r0000gn/T/ipykernel_82570/4011217787.py:31: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat([results_df, pd.DataFrame({'File': [filename], 'Text': temp_df['Text'].iloc[0], 'Toxicity': [toxicity_score]})], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          File                                               Text  Toxicity\n",
      "0   nov_20.txt  'That's all right.' Kehinde, too, started to u...    0.0870\n",
      "1   nov_08.txt  The Fisherman of Iboko\\nTen yards of the fines...    0.1971\n",
      "2   nov_09.txt  \"That one called Babby is a wise owl, you know...    0.0673\n",
      "3   nov_19.txt  Threshold of Madness\\nGOT BOTH OF YOU AT LAST!...    0.0232\n",
      "4   nov_18.txt  What the Spirit Said\\nOn the seventh second of...    0.0606\n",
      "5   nov_01.txt  A short story by Muhd Ibrahim Muhsin\\nGwaggo c...    0.0709\n",
      "6   nov_15.txt  The two lay exhausted on the massive bed. The ...    0.2251\n",
      "7   nov_14.txt  \"You must be out of your mind!\" Sade shouted a...    0.0322\n",
      "8   nov_16.txt  Mazi Ogaranya, the 'Agada-gbachiri-uzo and one...    0.0298\n",
      "9   nov_02.txt  I REMEMBER GETTING HOME THAT DAY AND SEEING MA...    0.1633\n",
      "10  nov_03.txt  Back to the land of the animals\\nThe lion was ...    0.0426\n",
      "11  nov_17.txt  Chimezie and Chuby planned for the first exhib...    0.1424\n",
      "12  nov_13.txt  Windfall\\nThe news on the national radio was c...    0.1138\n",
      "13  nov_07.txt  My Early Childhood\\nShouldn't my mother, Dejub...    0.0526\n",
      "14  Nov_06.txt  Battles are generally won with morale. How mor...    0.0456\n",
      "15  nov_12.txt  Doors\\nFingers of smoke infiltrated the room. ...    0.1349\n",
      "16  nov_04.txt  James\\nJanuary 1997\\n\"Erika?â€œ\\nHe looked so di...    0.0857\n",
      "17  nov_10.txt  Both Chike and Bisi come from Wazobia. Chike i...    0.1783\n",
      "18  nov_11.txt  The era of monumental change or the monumental...    0.4851\n",
      "19  Nov_05.txt  In time I became one of Sawa's best friends an...    0.0943\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "\n",
    "# Specify the folder path containing text files\n",
    "folder_path = 'ICE-Nigeria-txt-and-xml-files_version_Nov_03_2015/ice-nig/txt - without speaker tags/written/novels/'\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['File', 'Text', 'Toxicity'])\n",
    "\n",
    "# Iterate through each text file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the text from the file into a temporary DataFrame\n",
    "        temp_df = pd.DataFrame({'Text': [read_text_from_file(file_path)]})\n",
    "\n",
    "        # Compute toxicity for each text in the temporary DataFrame\n",
    "        results = toxicity.compute(predictions=temp_df['Text'].tolist())\n",
    "        toxicity_score = round(results[\"toxicity\"][0], 4)\n",
    "\n",
    "        # Append results to the main DataFrame\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({'File': [filename], 'Text': temp_df['Text'].iloc[0], 'Toxicity': [toxicity_score]})], ignore_index=True)\n",
    "\n",
    "# Print the DataFrame\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default facebook/roberta-hate-speech-dynabench-r4-target checkpoint\n"
     ]
    },
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'transcript_deepcut.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m/Users/isab7070/Documents/python_scripts/toxicity detection.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isab7070/Documents/python_scripts/toxicity%20detection.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m results_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mFile\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mToxicity\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isab7070/Documents/python_scripts/toxicity%20detection.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Iterate through each text file in the folder\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/isab7070/Documents/python_scripts/toxicity%20detection.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mfor\u001b[39;00m filename \u001b[39min\u001b[39;00m os\u001b[39m.\u001b[39;49mlistdir(folder_path):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isab7070/Documents/python_scripts/toxicity%20detection.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39mif\u001b[39;00m filename\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/isab7070/Documents/python_scripts/toxicity%20detection.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         file_path \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(folder_path, filename)\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'transcript_deepcut.txt'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import evaluate\n",
    "import os\n",
    "\n",
    "def read_text_from_file(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "toxicity = evaluate.load(\"toxicity\", module_type=\"measurement\")\n",
    "\n",
    "# Specify the folder path containing text files\n",
    "folder_path = 'transcript_deepcut.txt'\n",
    "#folder_path = 'ICE-Nigeria-txt-and-xml-files_version_Nov_03_2015/ice-nig/txt - without speaker tags/spoken all/'\n",
    "\n",
    "# Create an empty DataFrame to store results\n",
    "results_df = pd.DataFrame(columns=['File', 'Toxicity'])\n",
    "\n",
    "# Iterate through each text file in the folder\n",
    "for filename in os.listdir(folder_path):\n",
    "    if filename.endswith(\".txt\"):\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Read the text from the file into a temporary DataFrame\n",
    "        temp_df = pd.DataFrame({'Text': [read_text_from_file(file_path)]})\n",
    "\n",
    "        # Compute toxicity for each text in the temporary DataFrame\n",
    "        results = toxicity.compute(predictions=temp_df['Text'].tolist())\n",
    "        toxicity_score = round(results[\"toxicity\"][0], 4)\n",
    "\n",
    "        # Append results to the main DataFrame\n",
    "        results_df = pd.concat([results_df, pd.DataFrame({'File': [filename], 'Toxicity': [toxicity_score]})], ignore_index=True)\n",
    "\n",
    "# Save the results to a CSV file\n",
    "results_df.to_csv('toxicity_results.csv', index=False)\n",
    "\n",
    "# Display the DataFrame\n",
    "#print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mynlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
